Let us apply the Reynolds operator with respect to an action on concrete polynomials.
Before we look at finite generators of Hilbert's nullcone (which we will talk about later), we will just look at generators of the polynomial ring.

\begin{example}
  Consider the group $G = \operatorname{SL}_2$ and the vector space $ V = \left\{ \, A \in \mathbb{R}^{2 \times 2} \mid A^T = A \, \right\} $.
  Now we will look at the following action:
  \begin{equation}
    \begin{aligned}
      \mu \colon && \operatorname{SL}_2 & \times & V && \longrightarrow & V \\
      & ( & S & , &  A & ) & \longmapsto & SAS^T
    \end{aligned}
  \end{equation}
  Now consider the following for $S \in \operatorname{SL}_2$ and $A \in V$:
  \begin{equation}
    \begin{aligned}
      S &=
      \begin{bmatrix}
        s_{1,1} & s_{1,2} \\
        s_{2,1} & s_{2,2}
      \end{bmatrix}
      & A &=
      \begin{bmatrix}
        a_{1,1} & a_{1,2} \\
        a_{2,1} & a_{2,2}
      \end{bmatrix}
      \\
      S^{-1} &=
      \begin{bmatrix}
        s_{2,2} & -s_{1,2} \\
        -s_{2,1} & s_{1,1}
      \end{bmatrix}
    \end{aligned}
  \end{equation}
  We then have
  \begin{equation}
    \begin{aligned}
      &S^{-1}.A = S^{-1}A\left(S^{-1}\right)^T  \\ &=
      \begin{bmatrix}
        a_{1,1}s_{2,2}^2 - 2a_{1,2}s_{1,2}s_{2,2} + a_{2,2}s_{1,2}^2 & -a_{1,1}s_{2,1}s_{2,2} + a_{1,2}s_{1,1}s_{2,2} + a_{1,2}s_{1,2}s_{2,1} - a_{2,2}s_{1,1}s_{1,2} \\
        - a_{1,1}s_{2,1}s_{2,2} + a_{1,2}s_{1,1}s_{2,2} + a_{1,2}s_{1,2}s_{2,1} - a_{2,2}s_{1,1}s_{1,2} & a_{1,1}s_{2,1}^2 - 2a_{1,2}s_{1,1}s_{2,1} + a_{2,2}s_{1,1}^2
      \end{bmatrix}
    \end{aligned}
  \end{equation}
  Notice that we also have
  \begin{equation}
    \begin{aligned}
      &\operatorname{det}\left( \frac{\partial}{\partial S} \right)^n
      &&= \left( \frac{\partial}{\partial S_{1,1}} \frac{\partial}{\partial S_{2,2}} - \frac{\partial}{\partial S_{1,2}} \frac{\partial}{\partial S_{2,1}} \right)^n \\
      &&&= \sum_{k=0}^n (-1)^k \binom{n}{k} \frac{\partial}{\partial S_{1,1}}^{n-k} \frac{\partial}{\partial S_{1,2}}^k \frac{\partial}{\partial S_{2,1}}^k \frac{\partial}{\partial S_{2,2}}^{n-k}
    \end{aligned}
  \end{equation}
  It is quite cumbersome to calculate the Reynolds Operator of general polynomials.
  We will look at the monomial $A_{1,1}^2$, for which we have
  \begin{equation}
    \begin{aligned}
      &&&\bar{mu}^\prime (A_{1,1}^2) \\
      &&=& S_{2,2}^4 \otimes A_{1,1}^2 - 4S_{1,2}S_{2,2}^3 \otimes A_{1,1}A_{1,2} + 2S_{1,2}^2 S_{2,2}^2 \otimes A_{1,1}A_{2,2} \\
      &&& + 4S_{1,2}^2 S_{2,2}^2 \otimes A_{1,2}^2 - 4S_{1,2}^3 S_{2,2} \otimes A_{1,2}A_{2,2} + S_{1,2}^4 \otimes A_{2,2}^2\\
      &&=& \frac{S_{2,2}^4}{\operatorname{det}(S)^2} \otimes A_{1,1}^2 - \frac{4S_{1,2}S_{2,2}^3}{\operatorname{det}(S)^2} \otimes A_{1,1}A_{1,2} \\
      &&& + \frac{2S_{1,2}^2 S_{2,2}^2}{\operatorname{det}(S)^2} \otimes A_{1,1}A_{2,2} + \frac{4S_{1,2}^2 S_{2,2}^2}{\operatorname{det}(S)^2} \otimes A_{1,2}^2 \\
      &&& - \frac{4S_{1,2}^3 S_{2,2}}{\operatorname{det}(S)^2} \otimes A_{1,2}A_{2,2} + \frac{S_{1,2}^4}{\operatorname{det}(S)^2} \otimes A_{2,2}^2
    \end{aligned}
  \end{equation}
  We can now apply the Reynolds operator in the way we discussed it in proposition \ref{ro} in combination with Cayley's $\Omega$-process.
  Since all terms in $K[\operatorname{SL}_2]$ are already of degree $2$, apply the same derivatives to each summand and calculate:
  \begin{equation}
    \begin{aligned}
      & R_G \cdot A_{1,1}^2 \\
      =& \left( \frac{\partial}{\partial S_{1,1}}^2 \frac{\partial}{\partial S_{2,2}}^2 - 2 \frac{\partial}{\partial S_{1,1}} \frac{\partial}{\partial S_{1,2}} \frac{\partial}{\partial S_{2,1}} \frac{\partial}{\partial S_{2,2}} + \frac{\partial}{\partial S_{1,2}}^2\frac{\partial}{\partial S_{2,1}}^2 \right) \cdot A_{1,1}^2 \\
      =& 0
    \end{aligned}
  \end{equation}
  The zero-polynomial is not very interesting, so applying the Reynolds Operator to any polynomial will not always produce interesting results.
  We will try again for the polynomial $A_{1,2}^2$.
  We calculate
  \begin{equation}
    \begin{aligned}
      &\mu^\prime ( A_{1,2}^2 ) \\
      =& S_{2,1}^2 S_{2,2}^2 \otimes A_{1,1}^2
      - 2S_{1,1}S_{2,1}S_{2,2}^2 \otimes A_{1,1}A_{1,2} \\
      &- 2S_{1,2}S_{2,1}^2S_{2,2} \otimes A_{1,2}^2 
      + 2S_{1,1}S_{1,2}S_{2,1}S_{2,2} \otimes A_{1,1}A_{2,2}\\
      &+ S_{1,1}^2S_{2,2}^2 \otimes A_{1,2}^2
      + 2S_{1,1}S_{1,2}S_{2,1}S_{2,2} \otimes A_{1,2}^2 \\
      &- 2S_{1,1}^2S_{1,2}S_{2,2} \otimes A_{1,2}A_{2,2}
      + S_{1,2}^2S_{2,1}^2 \otimes A_{1,2}^2 \\
      &- 2S_{1,1}S_{1,2}^2S_{2,1} \otimes A_{1,2}A_{2,2}
      + S_{1,1}^2S_{1,2}^2 \otimes A_{2,2}^2\\
      =& \frac{S_{2,1}^2 S_{2,2}^2}{\operatorname{det}(S)^2} \otimes A_{1,1}^2
      - \frac{2S_{1,1}S_{2,1}S_{2,2}^2}{\operatorname{det}(S)^2} \otimes A_{1,1}A_{1,2} \\
      &- \frac{2S_{1,2}S_{2,1}^2S_{2,2}}{\operatorname{det}(S)^2} \otimes A_{1,2}^2 
      + \frac{2S_{1,1}S_{1,2}S_{2,1}S_{2,2}}{\operatorname{det}(S)^2} \otimes A_{1,1}A_{2,2}\\
      &+ \frac{S_{1,1}^2S_{2,2}^2}{\operatorname{det}(S)^2} \otimes A_{1,2}^2
      + \frac{2S_{1,1}S_{1,2}S_{2,1}S_{2,2}}{\operatorname{det}(S)^2} \otimes A_{1,2}^2 \\
      &- \frac{2S_{1,1}^2S_{1,2}S_{2,2}}{\operatorname{det}(S)^2} \otimes A_{1,2}A_{2,2}
      + \frac{S_{1,2}^2S_{2,1}^2}{\operatorname{det}(S)^2} \otimes A_{1,2}^2 \\
      &- \frac{2S_{1,1}S_{1,2}^2S_{2,1}}{\operatorname{det}(S)^2} \otimes A_{1,2}A_{2,2}
      + \frac{S_{1,1}^2S_{1,2}^2}{\operatorname{det}(S)^2} \otimes A_{2,2}^2\\      
    \end{aligned}
  \end{equation}
  Again, all $K[\operatorname{SL}_2]$ terms are of degree $2$, therefore we can simplify and calculate
  \begin{equation}
    \begin{aligned}
      &R_G \cdot A_{1,2}^2\\
      =& \left( \frac{\partial}{\partial S_{1,1}}^2 \frac{\partial}{\partial S_{2,2}}^2 - 2 \frac{\partial}{\partial S_{1,1}} \frac{\partial}{\partial S_{1,2}} \frac{\partial}{\partial S_{2,1}} \frac{\partial}{\partial S_{2,2}} + \frac{\partial}{\partial S_{1,2}}^2\frac{\partial}{\partial S_{2,1}}^2 \right) \cdot A_{1,2}^2 \\
      =& - \frac{4}{12} A_{1,1}A_{2,2} + \frac{4}{12} A_{1,2}^2 - \frac{4}{12} A_{1,2}^2 + \frac{4}{12} A_{1,2}^2 \\
      =& -\frac{1}{3}\operatorname{det}(A)
    \end{aligned}
  \end{equation}
  This is in line with what we expect: $K[V]^{\operatorname{SL}_n} = K[\operatorname{det}(A)]$.
  % Now consider a monomial $ f = A_{1,1}^{r_{1,1}} A_{1,2}^{r_{1,2}} A_{2,2}^{r_{2,2}} \in K[V] $.
  % We then have
  % \begin{equation}
  %   \begin{aligned}
  %     & \mu^\ast (f) \\
  %     =& \sum_{|t_{1,1}| = r_{1,1}} \, \sum_{|t_{1,2}| = r_{1,2}} \, \sum_{|t_{2,2}| = r_{2,2}} \binom{r_{1,1}}{t_{1,1}} \binom{r_{1,2}}{t_{1,2}} \binom{r_{2,2}}{t_{2,2}}\\
  %     =& \sum s_{1,1}^{ t_{1,2}^{(2)} + t_{1,2}^{(4)} + t_{2,2}^{(2)} + 2t_{2,2}^{(3)}}
  %   \end{aligned}
  % \end{equation}
\end{example}

\begin{example}
  We will now discuss the cross ratio.
  Since the projective line isn't an affine variety, we will look at points in $K^2$ to make the situation affine, which will make some things different.
  Consider $(K^2)^4$ and with the coordinate functions $\{(X_i)_k\}_{i \in [4], k \in [2]}$.
  (We write $X_i = \binom{(X_i)_1}{(X_i)_2}$ for $i \in [4]$.)
  Define $q := \prod_{i,j \in [r], i<j} \operatorname{det}(X_i,X_j)$.
  As described in \ref{rabbi}, we have an affine variety
  \begin{equation}
    % \begin{aligned}
    %   &X&:=&\{\, (a,b,c,d) \in (K^2)^4 \mid (b_1c_2 - b_2c_1)(d_1a_2 - d_2a_1) \neq 0 \,\}\\
    %   &&=& \{\, (a,b,c,d) \in (K^2)^4 \mid \operatorname{det}(b,c)\operatorname{det}(d,a) \neq 0 \,\}
    % \end{aligned}
    X := \{\, (x_1,x_2,x_3,x_4) \in (K^2)^4 \mid q(x_1,x_2,x_3,x_4) \neq 0 \,\}
  \end{equation}
  with the coordinate ring $K[X] = K[\{(X_i)_k\}_{i \in [4], k \in [2]},q^{-1}]$.
  Nothe that $q(x_1,x_2,x_3,x_4) \neq 0$ is equivalent to saying that for $i\neq j$ we have $x_i \notin \operatorname{span}{x_j}$, or rather in projective terms $[x_i] \neq [x_j]$.
  Now consider the action of $\operatorname{GL}_2$ on $X$ via pointwise application.
  The \textit{cross ratio} $\operatorname{cr} \in K[X]$ defined as follows
  \begin{equation}
    \begin{aligned}
      \operatorname{cr} \colon&&X&\longrightarrow K \\
      &&(x_1,x_2,x_3,x_4) &\longmapsto \frac{\operatorname{det}(x_1,x_2)\operatorname{det}(x_3,x_4)}{\operatorname{det}(x_2,x_3)\operatorname{det}(x_4,x_1)}
    \end{aligned}
  \end{equation}
  is an invariant under this action, as well as the same map with permuted inputs, as is easily seen.
  Together with the fact that the condition $q(x_1,x_2,x_3,x_4) \neq 0$ is also $\operatorname{GL}_2$-invariant, this actually guarantees us a coordinate-free definition of the cross ratio for any two-dimensional vector-space.
  The invariant ring $K[X]^{\operatorname{GL}_n}$ is finitely generated, as we know by Hilbert's finiteness theorem.
  In fact, we don't have more invariants than polynomials in the cross ratio: $K[X]^{\operatorname{GL}_n} = K[\{\operatorname{cr}(X_{\pi_1},X_{\pi_2},X_{\pi_3},X_{\pi_4})\}_{\pi \in S_4}]$.
  We actually have $K[\{\operatorname{cr}(X_{\pi_1},X_{\pi_2},X_{\pi_3},X_{\pi_4})\}_{\pi \in S_4}] = K[\operatorname{cr},(\operatorname{cr}(\operatorname{cr} -1))^{-1}] $, since we can calculate that for any permutation $\pi in S_4$ we have $ \operatorname{cr}(X_{\pi_1},X_{\pi_2},X_{\pi_3},X_{\pi_4}) \in \left\{ \operatorname{cr}, \operatorname{cr}^{-1}, 1 - \operatorname{cr}, (1-\operatorname{cr})^{-1}, \operatorname{cr}^{-1}(\operatorname{cr}-1)\right\}$.
  % This might look surprising because the cross ratio with permuted inputs doesn't seem like it is included, and also $(\operatorname{cr}(\operatorname{cr} -1))^{-1}$ doesn't look like it is an element of the coordinate ring $K[X]$.
  % But these are just illusions (pardon my playful phrasing):
  % We can calculate that for $x_1,x_2,x_3,x_4 \in X$ we have $\operatorname{cr}(x_1,x_2,x_3,x_4) = \operatorname{cr}(x_1,x_2,x_4,x_3)^{-1} = 1 - \operatorname{cr}(x_1,x_3,x_2,x_4) = 1 - \operatorname{cr}(x_1,x_3,x_4,x_2)^{-1} = (1 - \operatorname{cr}(x_1,x_4,x_2,x_3))^{-1} = (1 - \operatorname{cr}(x_1,x_4,x_3,x_2)^{-1})^{-1}$.
  % All other permutations of the inputs are one of these six terms.
  To see that the invariant ring looks like what we claimed, we need some results from the projective geometry.
  The cross ratio is also defined in the projective space:
  Let $Y$ be defined as those points in $P(K^2)^4$ that are pairwise distinct.
  $Y$ is equal to $P(X)$, and $X$ itself is already a cone.
  The projective cross ratio $\operatorname{cr}_P$ is then well-defined since it is independent of the choice of representatives because the determinant is multilinear.
  This means that for any $ (a,b,c,d) \in X $ we are allowed to write $\operatorname{cr}(a,b,c,d) = \operatorname{cr}([a],[b],[c],[d]) $.
  For the same reasons as before, this projective cross ratio is also $\operatorname{PGL}(K^2)$-invariant.
  If $x_1,x_2,x_3,y_1,y_2,y_3 \in P(K^2)$ with $x_1$, $x_2$ and $x_3$ pairwise distinct and pairwise $y_1$, $y_2$ and $y_3$ distinct, then an important theorem in projective geometry is that there exists a (unique) projective transformation $ \rho \in \operatorname{PGL}(K^2)$ such that $\rho (x_1) = y_1$, $\rho(x_2) = y_2$ and $\rho(x_3) = y_3$.
  Let $A,B,C,D \in Y$, which implies that $B,C,D$ are pairwise distinct.
  For $x \in K $ we define $x_P := \left[\binom{x}{1}\right]$ and $\infty_P := \left[\binom{1}{0}\right]$.
  There then exists a $\rho_{B,C,D} \in \operatorname{PGL}(K^2)$ such that $\rho_{B,C,D} (B) = 0_P$, $\rho_{B,C,D} (C) = 1_P$ and $\rho_{B,C,D} (D) = \infty_P$.
  Since $A$ is distinct from $D$ we know $\rho_{B,C,D} (A) \neq \infty_P$, and therefore there exists some $a \in K$ such that $\rho_{B,C,D}(A) = [\binom{a}{1}]$.
  We then compute $\rho_{B,C,D} (A) = [\binom{a}{1}] = \operatorname{cr}([\binom{a}{1}],[\binom{0}{1}],[\binom{1,1}],[\binom{1,0}])_P = \operatorname{cr}(\rho_{B,C,D}(A),\rho_{B,C,D}(B),\rho_{B,C,D}(C),\rho_{B,C,D}(D))_P = \operatorname{cr}(A,B,C,D)_P$.
  This means that for $(a,b,c,d) \in X$ we have $\rho_{[b],[c],[d]} ([a]) = \operatorname{cr}(a,b,c,d)$, so we can choose a representative $r_{a,b,c,d} \in \rho_{[b],[c],[d]}$ such that $r_{a,b,c,d}(a) = \binom{\operatorname{cr}(a,b,c,d)}{1}$.
  Let $f \in K[X]^{\operatorname{GL}_2}$.
  We have $f(a,b,c,d) = r_{a,b,c,d}^{-1}.f (a,b,c,d) = f(r_{a,b,c,d}(a),r_{a,b,c,d}(b),r_{a,b,c,d}(c),r_{a,b,c,d}(d)) = $
\end{example}

\begin{example}
  % \textbf{($K$ needs to be algebraically closed for Zariski-denseness of diagonizable matrices...)}
  Let $K$ be an algebraically-closed field.
  Consider $\operatorname{GL}_n$ viewed as the group of all change-of-coordinates transformations for endomorphisms on $K^n$, that is the rational representation
  \begin{equation}
    \begin{aligned}
      \mu \colon && \operatorname{GL}_n \times K^{n.n} & \longrightarrow K^{n,n} \\
      && (\sigma,A) &\longmapsto \sigma A \sigma^{-1}
    \end{aligned}
  \end{equation}
  What are its invariants?
  The invariants are exaclty those polynomials that are independent of the choice of the basis.
  The most well-known invariant is the determinant.
  From this obvservation we can find even more:
  We can follow that the characteristic polynomial of a matrix $A$, that is $\operatorname{det} (tI_n - A)$, does not change under a change of coordinates.
  If we write
  \begin{equation}
    \begin{aligned}
      \operatorname{det} (tI_n - A) = \sum_{i=0}^n p_{n,i} (A) t^i
    \end{aligned}
  \end{equation}
  this means that every $p_{n,i}$ is an invariant polynomial in $K[K^{n.n}]$!
  This is how one usually proves that the trace is an invariant polynomial after observing that $p_{n,n-1} = \operatorname{tr}_n$.
  Are there other invariants than these $p_{n,i}$?
  No!
  To see this, we will use a little trick:
  Consider $D := \{\, \delta \in K^{n,n} \mid \delta \operatorname{diagonalizable} \,\} \subseteq K[K^{n,n}]$.
  Since $K$ is algebraically closed, $D$ is Zariski-dense in $K^{n,n}$, and we have $K[K^{n,n}] \simeq = \left. K[K^{n,n}] \right|_{D}$ via $p \leftrightarrow \left. p \right|_{D}$.
  For this reason, we will look at the evaluation of an invariant polynomial $p \in K[K^{n.n}] $ only on elements in $D$, and can deduce what polynomial it is.

  Let $p \in K[K^{n,n}]^{\operatorname{GL_n}}$.
  We define a projection onto the diagonal:
  $\pi \colon K^{n,n} \twoheadrightarrow K^n , [A_{i,j}]_{i,j \in [n]} \longmapsto (A_{i,i})_{i \in [n]} $.
  % We also define an inclusion of $K^n$ into $K^{n,n}$:
  % $\iota \colon K^n \hookrightarrow K^{n,n}$, $(A_i)_{i \in [n]} \mapsto $
  % Now, for $A \in D$, there exists a $\sigma_A \in \operatorname{GL}_n$ such that $\sigma_A . A $ is diagonal.
  % If $\tilde{p} = p \left( [ \delta_{i,j} Z_{i,j}]_{i,j \in [n]} \right) \in K[K^n]$, then, since $p$ is an invariant, for all $A \in D$ we have
  % \begin{equation}
  %   \begin{aligned}
  %     &p(A)&=& \sigma_A^{-1} . p (A)\\
  %     &&=& p(\sigma_A .A)\\
  %     &&=&  \tilde{p} ( \pi  (\sigma_A . A))
  %   \end{aligned}
  % \end{equation}
  % As long as $\sigma .A$ is diagonal, this is true regardless of the choice of $\sigma_A$.
  Consider $\tilde{p} := p \circ \operatorname{diag}_n$
  $\tilde{p}$ is $S_n$-invariant:
  If $M_\tau \in \operatorname{GL}_n$ is the permutation matrix corresponding to $\tau \in S_n$, then for all $\tau \in S_n$ and for all $ X \in K^n$ we have
  \begin{equation}
    \begin{aligned}
      &\tau.\tilde{p} (X) &=& \tilde{p} (\tau^{-1}.X)\\
      &&=& p ( \operatorname{diag}_n(\tau^{-1}.X)\\
      &&=& p( M_\tau^{-1} \cdot \operatorname{diag}_n (X))\\
      &&=& M_\tau . p (\operatorname{diag}_n(X))\\
      &&=& p (\operatorname{diag}_n(X))&=& \tilde{p} (X)
    \end{aligned}
  \end{equation}
  From the fundamental theorem of symmetric polynomials we can follow that $\tilde{p} \in \operatorname{span}\{e_{n,i}\}_{i=0}^n$, say $\tilde{p} = \Sigma_{i=0}^n \lambda_i e_{n,i}$, where $\{ e_{n,i} \}_{i=0}^n$ are the elementary symmetric polynomials of dimension $n$.
  Now, for a choice (!) of $\sigma_A \in \operatorname{GL}_n$ such that $\sigma_A . A$ is diagonal, we easily see that for $s(A) := \sigma_A . A$ we get $p = p \circ s = \tilde{p} \circ \pi \circ s$, therefore $p = \Sigma_{i=0}^n \lambda_i e_{n,i} \circ \pi \circ s$.
  Now we want to show that $e_{n,i} \circ \pi \circ s = p_{n,i}$, which would conclude our claim.
  For all $A\in D$ we have
  \begin{equation}
    \begin{aligned}
      &\sum_{i=0}^n (e_{n,i} \circ \pi \circ s)(A)t^i&=&\operatorname{det}(t-\sigma_A.A)\\
      &&=&\operatorname{det}(t-A)
      &=&\sum_{i=0}^n p_{n,i}(A)t^i\\
    \end{aligned}
  \end{equation}
  which shows our claim.
  Note that this is independent of the choice of $s$, which means that we don't need the axiom of choice (rigorously, as usual, rewrite $s$ as a relation for all possible $s$ instead of a choice of a function\ldots).
\end{example}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bachelorarbeit"
%%% End:
